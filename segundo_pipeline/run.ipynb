{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e47eb534-f4c3-4503-8f8c-3d74282423f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp.dsl import Output, Dataset\n",
    "\n",
    "# PRIMEIRO COMPONENTE --> LOAD DATA\n",
    "@dsl.component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas\", \"gcsfs\"]\n",
    ")\n",
    "def load_train_data(\n",
    "    train_path: str,\n",
    "    train_csv: Output[Dataset]\n",
    "):\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(train_path)\n",
    "    df.to_csv(train_csv.path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab253149-a375-46bc-af0d-c6f077081524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp.dsl import Input, Output, Dataset, Artifact\n",
    "\n",
    "\n",
    "# SEGUNDO COMPONENTE DIVISÃO EM X E Y\n",
    "@dsl.component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\", \"gcsfs\"]\n",
    ")\n",
    "def preprocess_encode(\n",
    "    train_csv: Input[Dataset],       # entrada: CSV do componente anterior\n",
    "    X_csv: Output[Dataset],          # saída: features\n",
    "    y_csv: Output[Dataset],          # saída: target\n",
    "    encoders_json: Output[Artifact]  # saída: dicionário dos encoders\n",
    "):\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    import json\n",
    "\n",
    "    print(f\"Lendo dataset: {train_csv.path}\")\n",
    "    df = pd.read_csv(train_csv.path)\n",
    "\n",
    "    target = \"accident_risk\"\n",
    "    categorical_cols = [\n",
    "        \"road_type\", \"lighting\", \"weather\", \"time_of_day\",\n",
    "        \"holiday\", \"school_season\", \"public_road\", \"road_signs_present\"\n",
    "    ]\n",
    "\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[[target]]\n",
    "\n",
    "    encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "        encoders[col] = dict(\n",
    "            zip(le.classes_.tolist(), le.transform(le.classes_).tolist())\n",
    "        )\n",
    "\n",
    "    print(\"Salvando arquivos de saída...\")\n",
    "    X.to_csv(X_csv.path, index=False)\n",
    "    y.to_csv(y_csv.path, index=False)\n",
    "    with open(encoders_json.path, \"w\") as f:\n",
    "        json.dump(encoders, f)\n",
    "\n",
    "    print(\"✅ Pré-processamento concluído!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cfbd8c7-1404-49bf-b639-e4d8d68fb6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TERCEIRO COMPONENTE DIVISÃO DOS ARQUIVOS\n",
    "\n",
    "from kfp import dsl\n",
    "from kfp.dsl import Input, Output, Dataset\n",
    "\n",
    "@dsl.component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\", \"gcsfs\"]\n",
    ")\n",
    "def train_val_split(\n",
    "    X_csv: Input[Dataset],\n",
    "    y_csv: Input[Dataset],\n",
    "    X_train_csv: Output[Dataset],\n",
    "    X_val_csv: Output[Dataset],\n",
    "    y_train_csv: Output[Dataset],\n",
    "    y_val_csv: Output[Dataset],\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "):\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    print(f\"Lendo dados de entrada:\")\n",
    "    print(f\" - X: {X_csv.path}\")\n",
    "    print(f\" - y: {y_csv.path}\")\n",
    "\n",
    "    X = pd.read_csv(X_csv.path)\n",
    "    y = pd.read_csv(y_csv.path)\n",
    "\n",
    "    print(f\"Tamanho inicial: X={X.shape}, y={y.shape}\")\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    print(f\"Salvando artefatos de saída...\")\n",
    "    X_train.to_csv(X_train_csv.path, index=False)\n",
    "    X_val.to_csv(X_val_csv.path, index=False)\n",
    "    y_train.to_csv(y_train_csv.path, index=False)\n",
    "    y_val.to_csv(y_val_csv.path, index=False)\n",
    "\n",
    "    print(f\"✅ Divisão concluída! Treino: {X_train.shape}, Validação: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7206f1c3-e82a-49eb-9bb7-4577abcb2403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp.dsl import Input, Output, Dataset, Artifact, Model, Metrics\n",
    "\n",
    "# --- DEFINIÇÃO DOS COMPONENTES ---\n",
    "# ... (código existente) ...\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "# Removida a importação de 'google.cloud.aiplatform' pois não é mais necessária\n",
    "\n",
    "\n",
    "@dsl.component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas\", \"xgboost\", \"scikit-learn\", \"joblib\", \"gcsfs\"]\n",
    ")\n",
    "def train_model(\n",
    "    X_train_csv: Input[Dataset],\n",
    "    y_train_csv: Input[Dataset],\n",
    "    model_output: Output[Model],\n",
    "    train_metrics: Output[Metrics],  # Alterado de dsl.OutputPath(str)\n",
    "    n_estimators: int = 200,\n",
    "    learning_rate: float = 0.1,\n",
    "    max_depth: int = 5,\n",
    "    subsample: float = 0.8,\n",
    "    colsample_bytree: float = 0.8,\n",
    "    random_state: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Treina um modelo XGBoost Regressor e salva o artefato do modelo\n",
    "    e as métricas de treino.\n",
    "    \"\"\"\n",
    "    # --- IMPORTAÇÕES NECESSÁRIAS DENTRO DO COMPONENTE ---\n",
    "    import os\n",
    "    import json\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    from xgboost import XGBRegressor\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "    # ---------------------------------------------------\n",
    "\n",
    "    try:\n",
    "        print(\"Iniciando o componente train_model...\")\n",
    "\n",
    "        # 1) lê dados\n",
    "        print(f\"Lendo X_train de: {X_train_csv.path}\")\n",
    "        X_train = pd.read_csv(X_train_csv.path)\n",
    "        print(f\"Lendo y_train de: {y_train_csv.path}\")\n",
    "        y_train = pd.read_csv(y_train_csv.path).values.ravel()\n",
    "        print(\"Dados de treino lidos com sucesso.\")\n",
    "\n",
    "        # 2) treina\n",
    "        print(\"Iniciando o treinamento do modelo XGBRegressor...\")\n",
    "        model = XGBRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        print(\"Treinamento concluído.\")\n",
    "\n",
    "        # 3) salva modelo\n",
    "        model_save_path = model_output.path + \"/model.joblib\"\n",
    "        \n",
    "        # Garante que o diretório de saída exista antes de salvar\n",
    "        print(f\"Garantindo que o diretório de saída exista: {model_output.path}\")\n",
    "        os.makedirs(model_output.path, exist_ok=True) # <-- CORREÇÃO 1\n",
    "\n",
    "        print(f\"Salvando o modelo em: {model_save_path}\")\n",
    "        joblib.dump(model, model_save_path)\n",
    "        print(\"Modelo salvo com sucesso.\")\n",
    "\n",
    "        # 4) métricas simples no treino (só para tracking)\n",
    "        print(\"Calculando métricas de treino...\")\n",
    "        y_pred_tr = model.predict(X_train)\n",
    "        rmse_tr = float(np.sqrt(mean_squared_error(y_train, y_pred_tr)))\n",
    "        r2_tr = float(r2_score(y_train, y_pred_tr))\n",
    "        \n",
    "        metrics_data = {\"rmse_train\": rmse_tr, \"r2_train\": r2_tr}\n",
    "        print(f\"Métricas de treino: {metrics_data}\")\n",
    "\n",
    "        # Garante que o diretório para o arquivo de métricas exista\n",
    "        # Usa .path pois agora é um Artefato\n",
    "        print(f\"Garantindo que o diretório de métricas exista: {os.path.dirname(train_metrics.path)}\")\n",
    "        os.makedirs(os.path.dirname(train_metrics.path), exist_ok=True)\n",
    "\n",
    "        # Usa .path pois agora é um Artefato\n",
    "        with open(train_metrics.path, \"w\") as f:\n",
    "            json.dump(metrics_data, f, indent=4)\n",
    "        \n",
    "        print(\"Métricas de treino salvas. Componente train_model concluído.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro no componente train_model: {e}\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8271338f-4d2d-42b5-9fc9-0594920707dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas\", \"xgboost\", \"scikit-learn\", \"joblib\", \"gcsfs\"]\n",
    ")\n",
    "def evaluate_model(\n",
    "    model_input: Input[Model],\n",
    "    X_val_csv: Input[Dataset],\n",
    "    y_val_csv: Input[Dataset],\n",
    "    eval_metrics: Output[Metrics],      # Alterado de dsl.OutputPath(str)\n",
    "    predictions_csv: Output[Dataset],  # Alterado de dsl.OutputPath(str)\n",
    "):\n",
    "    \"\"\"\n",
    "    Avalia um modelo treinado em dados de validação.\n",
    "    \"\"\"\n",
    "    # --- IMPORTAÇÕES NECESSÁRIAS DENTRO DO COMPONENTE ---\n",
    "    import os\n",
    "    import json\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "    try:\n",
    "        print(\"Iniciando o componente evaluate_model...\")\n",
    "\n",
    "        # Carrega o modelo\n",
    "        model_load_path = model_input.path + \"/model.joblib\"\n",
    "        print(f\"Carregando modelo de: {model_load_path}\")\n",
    "        model = joblib.load(model_load_path)\n",
    "        print(\"Modelo carregado com sucesso.\")\n",
    "\n",
    "        # Carrega dados de validação\n",
    "        print(f\"Lendo X_val de: {X_val_csv.path}\")\n",
    "        X_val = pd.read_csv(X_val_csv.path)\n",
    "        print(f\"Lendo y_val de: {y_val_csv.path}\")\n",
    "        y_val = pd.read_csv(y_val_csv.path).values.ravel()\n",
    "        print(\"Dados de validação lidos com sucesso.\")\n",
    "\n",
    "        # Avaliação\n",
    "        print(\"Iniciando predições no conjunto de validação...\")\n",
    "        y_pred = model.predict(X_val)\n",
    "        rmse = float(np.sqrt(mean_squared_error(y_val, y_pred)))\n",
    "        mae = float(mean_absolute_error(y_val, y_pred))\n",
    "        r2 = float(r2_score(y_val, y_pred))\n",
    "        \n",
    "        metrics_data = {\"rmse\": rmse, \"mae\": mae, \"r2\": r2}\n",
    "        print(f\"Métricas de avaliação: {metrics_data}\")\n",
    "\n",
    "        # Garante que o diretório para o arquivo de métricas exista\n",
    "        # Usa .path pois agora é um Artefato\n",
    "        print(f\"Garantindo que o diretório de métricas exista: {os.path.dirname(eval_metrics.path)}\")\n",
    "        os.makedirs(os.path.dirname(eval_metrics.path), exist_ok=True)\n",
    "\n",
    "        # Usa .path pois agora é um Artefato\n",
    "        with open(eval_metrics.path, \"w\") as f:\n",
    "            json.dump(metrics_data, f, indent=4)\n",
    "        print(\"Métricas de avaliação salvas.\")\n",
    "\n",
    "        # Garante que o diretório para o arquivo de predições exista\n",
    "        # Usa .path pois agora é um Artefato\n",
    "        print(f\"Garantindo que o diretório de predições exista: {os.path.dirname(predictions_csv.path)}\")\n",
    "        os.makedirs(os.path.dirname(predictions_csv.path), exist_ok=True)\n",
    "\n",
    "        print(f\"Salvando predições em: {predictions_csv.path}\")\n",
    "        # Usa .path pois agora é um Artefato\n",
    "        pd.DataFrame({\"y_true\": y_val, \"y_pred\": y_pred}).to_csv(predictions_csv.path, index=False)\n",
    "        print(\"Predições salvas. Componente evaluate_model concluído.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        \n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4aea0e7-f842-46db-9ed5-c939dd7b4ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necessários para a *definição* do componente (ficam fora)\n",
    "from kfp.dsl import (\n",
    "    Input, \n",
    "    Model, \n",
    "    Metrics, \n",
    "    component\n",
    ")\n",
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-aiplatform\",  # SDK principal do Vertex AI\n",
    "        \"gcsfs\",\n",
    "        \"pandas\",\n",
    "        \"joblib\"\n",
    "    ]\n",
    ")\n",
    "def register_model_in_registry(\n",
    "    model_input: Input[Model],\n",
    "    eval_metrics: Input[Metrics],\n",
    "    project_id: str,\n",
    "    project_location: str,\n",
    "    model_display_name: str,      # Nome do modelo no Registry (ex: \"accident-risk-model\")\n",
    "):\n",
    "    \"\"\"\n",
    "    Registra um modelo treinado no Vertex AI Model Registry,\n",
    "    anexando as métricas de avaliação como labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Imports necessários para a *execução* do componente (ficam dentro)\n",
    "    import json\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    try:\n",
    "        print(f\"Iniciando o registro do modelo: {model_display_name}\")\n",
    "        print(f\"Projeto: {project_id}, Local: {project_location}\")\n",
    "        \n",
    "        # 1. Inicializa o cliente do Vertex AI\n",
    "        aiplatform.init(project=project_id, location=project_location)\n",
    "\n",
    "        # 2. Lê as métricas do arquivo JSON\n",
    "        # Usa .path pois agora é um Artefato\n",
    "        print(f\"Lendo métricas de: {eval_metrics.path}\")\n",
    "        with open(eval_metrics.path, \"r\") as f:\n",
    "            metrics_data = json.load(f)\n",
    "        \n",
    "        # Converte métricas em 'labels'\n",
    "        labels_for_registry = {}\n",
    "        for key, value in metrics_data.items():\n",
    "            # Garante que a chave seja compatível com labels (minúsculas, sem caracteres especiais)\n",
    "            safe_key = ''.join(c for c in key.lower() if c.isalnum() or c == '_')\n",
    "            # Garante que o valor seja compatível (string, substitui '.' por '_')\n",
    "            label_value = str(value).replace('.', '_').lower()\n",
    "            labels_for_registry[safe_key] = label_value\n",
    "\n",
    "        print(f\"Métricas convertidas para labels: {labels_for_registry}\")\n",
    "\n",
    "        # 3. Define o contêiner de predição\n",
    "        PREBUILT_CONTAINER_URI = \"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-7:latest\"\n",
    "        print(f\"Usando contêiner de predição: {PREBUILT_CONTAINER_URI}\")\n",
    "\n",
    "        # 4. Faz o upload (registro) do modelo\n",
    "        print(f\"Fazendo upload do modelo a partir de: {model_input.path}\")\n",
    "        \n",
    "        registered_model = aiplatform.Model.upload(\n",
    "            display_name=model_display_name,\n",
    "            artifact_uri=model_input.path,\n",
    "            serving_container_image_uri=PREBUILT_CONTAINER_URI,\n",
    "            description=f\"Versão registrada via KFP pipeline.\",\n",
    "            labels=labels_for_registry\n",
    "        )\n",
    "\n",
    "        print(f\"Modelo registrado com sucesso!\")\n",
    "        print(f\"Nome do recurso: {registered_model.resource_name}\")\n",
    "        print(f\"Versão: {registered_model.version_id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao registrar o modelo: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a3e538a-f154-4b00-aaa1-5acfccee3209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRIAÇÃO DO PIPELINE (VERSÃO ATUALIZADA)\n",
    "@dsl.pipeline(\n",
    "    name=\"accident-risk-pipeline\",\n",
    "    description=\"Pipeline com leitura, preprocessamento, split, treino, avaliação E REGISTRO\"\n",
    ")\n",
    "def accident_risk_pipeline(\n",
    "    train_path: str = \"gs://road_accident/train.csv\",\n",
    "    project_id: str = \"vertexai-457414\", # Adicione seu Project ID\n",
    "    project_location: str = \"us-central1\", # Adicione sua Localização (ex: \"us-central1\")\n",
    "    model_name: str = \"accident-risk-model\" # Nome que aparecerá no Registry\n",
    "):\n",
    "    \"\"\"\n",
    "    Pipeline completo que treina, avalia e registra\n",
    "    o modelo no Vertex AI Model Registry.\n",
    "    \"\"\"\n",
    "    \n",
    "    # NOTA: Esta pipeline assume que os componentes\n",
    "    # 'load_train_data', 'preprocess_encode', e 'train_val_split'\n",
    "    # estão definidos e importados em algum lugar.\n",
    "\n",
    "    # 1) carregar dados\n",
    "    load_task = load_train_data(train_path=train_path)\n",
    "    load_task.set_caching_options(False)   # força reexecução do primeiro passo\n",
    "\n",
    "    # 2) preprocessar e encodar\n",
    "    preprocess_task = preprocess_encode(train_csv=load_task.outputs[\"train_csv\"])\n",
    "\n",
    "    # 3) split train/val\n",
    "    split_task = train_val_split(\n",
    "        X_csv=preprocess_task.outputs[\"X_csv\"],\n",
    "        y_csv=preprocess_task.outputs[\"y_csv\"]\n",
    "    )\n",
    "\n",
    "    # 4) treinar modelo\n",
    "    # Esta função está definida ACIMA neste arquivo\n",
    "    train_task = train_model(\n",
    "        X_train_csv=split_task.outputs[\"X_train_csv\"],\n",
    "        y_train_csv=split_task.outputs[\"y_train_csv\"]\n",
    "        # você pode expor hiperparâmetros aqui se quiser\n",
    "    )\n",
    "\n",
    "    # 5) avaliar modelo\n",
    "    # Esta função está definida ACIMA neste arquivo\n",
    "    evaluate_task = evaluate_model(\n",
    "        model_input=train_task.outputs[\"model_output\"],  # Do train_model\n",
    "        X_val_csv=split_task.outputs[\"X_val_csv\"],       # do split\n",
    "        y_val_csv=split_task.outputs[\"y_val_csv\"]        # do split\n",
    "    )\n",
    "\n",
    "    # 6) --- NOVO PASSO: REGISTRAR O MODELO ---\n",
    "    # Esta função está definida ACIMA neste arquivo\n",
    "    register_task = register_model_in_registry(\n",
    "        model_input=train_task.outputs[\"model_output\"],          # O modelo treinado\n",
    "        eval_metrics=evaluate_task.outputs[\"eval_metrics\"],  # O JSON com as métricas\n",
    "        project_id=project_id,\n",
    "        project_location=project_location,\n",
    "        model_display_name=model_name\n",
    "    )\n",
    "    \n",
    "    # Garante que o registro só ocorra APÓS a avaliação\n",
    "    register_task.after(evaluate_task)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b70197b-aba7-4593-a405-3078c89b2d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import compiler\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=accident_risk_pipeline,\n",
    "    package_path=\"accident_risk_pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d83b777-159f-4bfa-85d1-235f758c7793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/666534829864/locations/us-central1/pipelineJobs/accident-risk-pipeline-20251023114635\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/666534829864/locations/us-central1/pipelineJobs/accident-risk-pipeline-20251023114635')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/accident-risk-pipeline-20251023114635?project=666534829864\n",
      "PipelineJob projects/666534829864/locations/us-central1/pipelineJobs/accident-risk-pipeline-20251023114635 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/666534829864/locations/us-central1/pipelineJobs/accident-risk-pipeline-20251023114635 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/666534829864/locations/us-central1/pipelineJobs/accident-risk-pipeline-20251023114635 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=\"vertexai-457414\", location=\"us-central1\")\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"accident-risk-pipeline-v1\",\n",
    "    template_path=\"accident_risk_pipeline.json\",\n",
    "    pipeline_root=\"gs://road_accident/pipelines\"\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e895ba-f313-4063-a7c7-479ae32a8858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496397b9-df2b-43e4-a5a8-1d663e7a0746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf898b4-320b-411d-9d41-368e9e5a67f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "roadaccident",
   "name": "workbench-notebooks.m134",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m134"
  },
  "kernelspec": {
   "display_name": "Python (road_accident)",
   "language": "python",
   "name": "road_accident"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
